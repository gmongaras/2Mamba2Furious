{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e5a3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85f335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(QK, M, V):\n",
    "    tmp = QK**2 * torch.exp(M)\n",
    "    A = tmp / tmp.sum(-1, keepdim=True)\n",
    "    return A @ V\n",
    "\n",
    "def manual_grad(QK, M, V):\n",
    "    QK_ = QK.clone().detach().requires_grad_()\n",
    "    M_ = M.clone().detach().requires_grad_()\n",
    "    V_ = V.clone().detach().requires_grad_()\n",
    "    out = forward(QK_, M_, V_)\n",
    "    out.sum().backward()\n",
    "    return QK_.grad, M_.grad, V_.grad\n",
    "\n",
    "class Squaremax(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, QK, M, V):\n",
    "        out = forward(QK, M, V)\n",
    "        ctx.save_for_backward(QK, M, V, out)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, prev_grad):\n",
    "        QK, M, V, out = ctx.saved_tensors\n",
    "        M_ = torch.exp(M)\n",
    "        tmp = QK**2 * M_\n",
    "        out = tmp / tmp.sum(-1, keepdim=True)\n",
    "        \n",
    "        do = prev_grad @ V.mT\n",
    "        \n",
    "        # Z = sum_j x_j^2 along the last dimension\n",
    "        Z = (tmp).sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # Using J^T v = (2/Z) * x * ( v - (vÂ·y) ), where y = out\n",
    "        v_dot_y = (do * out).sum(dim=-1, keepdim=True)\n",
    "        grad_tmp = (1 / Z) * M_ * (do - v_dot_y)\n",
    "        grad_QK = 2 * QK * grad_tmp\n",
    "        \n",
    "        # Due to the exponential, we keep M_ in grad_tmp. Without\n",
    "        # it, we would remvoe it for the M grad.\n",
    "        grad_M = grad_tmp * QK**2\n",
    "\n",
    "        return grad_QK, grad_M, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f9ab9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1 = 256\n",
    "dim2 = 256\n",
    "QK = torch.randn(dim1, dim2).cuda().double().detach().requires_grad_()\n",
    "mask = torch.tril(torch.ones(dim1, dim2)).bool().cuda().detach()\n",
    "M = (torch.randn(dim1, dim2).cuda().double() * mask).detach().requires_grad_()\n",
    "V = torch.randn(dim2, 128).cuda().double().detach().requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc694c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0547,  0.0172, -0.2347,  ..., -0.1243,  0.0523, -0.0247],\n",
       "        [ 0.0791, -0.1117, -0.0240,  ...,  0.0339, -0.0589,  0.0626],\n",
       "        [ 0.0352,  0.0638,  0.0155,  ...,  0.2923,  0.0679,  0.0813],\n",
       "        ...,\n",
       "        [-0.0088, -0.0155,  0.0156,  ...,  0.0343,  0.0423,  0.0812],\n",
       "        [ 0.0101, -0.0179,  0.0101,  ..., -0.1450,  0.0500,  0.0641],\n",
       "        [-0.0125,  0.0020,  0.0180,  ...,  0.1526, -0.0952, -0.0195]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QK_grad, M_grad, V_grad = manual_grad(QK, M, V)\n",
    "QK_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adb68763",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Squaremax.apply(QK, M, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b22f8a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b775e142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0547,  0.0172, -0.2347,  ..., -0.1243,  0.0523, -0.0247],\n",
       "        [ 0.0791, -0.1117, -0.0240,  ...,  0.0339, -0.0589,  0.0626],\n",
       "        [ 0.0352,  0.0638,  0.0155,  ...,  0.2923,  0.0679,  0.0813],\n",
       "        ...,\n",
       "        [-0.0088, -0.0155,  0.0156,  ...,  0.0343,  0.0423,  0.0812],\n",
       "        [ 0.0101, -0.0179,  0.0101,  ..., -0.1450,  0.0500,  0.0641],\n",
       "        [-0.0125,  0.0020,  0.0180,  ...,  0.1526, -0.0952, -0.0195]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QK_grad_ = QK.grad\n",
    "QK_grad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ba1be17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.8818e-16, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(QK_grad - QK_grad_).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c6ee67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3695e-02, -2.4378e-02, -1.1753e-02,  ..., -5.2944e-03,\n",
       "         -1.6745e-01, -5.7915e-02],\n",
       "        [-1.8094e-02, -9.2249e-02, -1.4489e-03,  ...,  4.9065e-04,\n",
       "         -5.6104e-01, -7.7263e-02],\n",
       "        [-1.8147e-03, -2.4071e-03, -1.2420e-02,  ..., -3.7256e-04,\n",
       "         -6.1703e-03, -9.8866e-03],\n",
       "        ...,\n",
       "        [-5.5734e-03, -4.5801e-04, -2.6770e-04,  ...,  6.0660e-04,\n",
       "         -1.6904e-02, -3.3013e-02],\n",
       "        [-4.3928e-06, -6.2730e-03, -1.3781e-02,  ..., -3.0220e-02,\n",
       "         -2.1008e-03, -8.8513e-03],\n",
       "        [-3.2837e-02, -4.8665e-03, -2.8812e-04,  ..., -1.0095e-03,\n",
       "         -5.0367e-02, -2.9068e-04]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5709b5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3695e-02, -2.4378e-02, -1.1753e-02,  ..., -5.2944e-03,\n",
       "         -1.6745e-01, -5.7915e-02],\n",
       "        [-1.8094e-02, -9.2249e-02, -1.4489e-03,  ...,  4.9065e-04,\n",
       "         -5.6104e-01, -7.7263e-02],\n",
       "        [-1.8147e-03, -2.4071e-03, -1.2420e-02,  ..., -3.7256e-04,\n",
       "         -6.1703e-03, -9.8866e-03],\n",
       "        ...,\n",
       "        [-5.5734e-03, -4.5801e-04, -2.6770e-04,  ...,  6.0660e-04,\n",
       "         -1.6904e-02, -3.3013e-02],\n",
       "        [-4.3928e-06, -6.2730e-03, -1.3781e-02,  ..., -3.0220e-02,\n",
       "         -2.1008e-03, -8.8513e-03],\n",
       "        [-3.2837e-02, -4.8665e-03, -2.8812e-04,  ..., -1.0095e-03,\n",
       "         -5.0367e-02, -2.9068e-04]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_grad_ = M.grad\n",
    "M_grad_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b420a69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.8818e-16, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(M_grad - M_grad_).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5914f641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/gmongaras/miniconda3/lib/python3.10/site-packages/torch/autograd/gradcheck.py:922: UserWarning: Input #1 requires gradient and is not a double precision floating point or complex. This check will likely fail if all the inputs are not of double precision floating point or complex. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 79.25 GiB of which 13.55 GiB is free. Process 1737049 has 988.00 MiB memory in use. Including non-PyTorch memory, this process has 64.72 GiB memory in use. Of the allocated memory 64.16 GiB is allocated by PyTorch, and 66.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/gradcheck.py:2055\u001b[0m, in \u001b[0;36mgradcheck\u001b[0;34m(func, inputs, eps, atol, rtol, raise_exception, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_batched_forward_grad, check_forward_ad, check_backward_ad, fast_mode, masked)\u001b[0m\n\u001b[1;32m   2053\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_gradcheck_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/gradcheck.py:2084\u001b[0m, in \u001b[0;36m_gradcheck_helper\u001b[0;34m(func, inputs, eps, atol, rtol, nondet_tol, check_undefined_grad, check_grad_dtypes, check_batched_grad, check_batched_forward_grad, check_forward_ad, check_backward_ad, fast_mode, masked)\u001b[0m\n\u001b[1;32m   2079\u001b[0m _check_outputs(outputs)\n\u001b[1;32m   2081\u001b[0m gradcheck_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m   2082\u001b[0m     _fast_gradcheck \u001b[38;5;28;01mif\u001b[39;00m fast_mode \u001b[38;5;28;01melse\u001b[39;00m _slow_gradcheck, masked\u001b[38;5;241m=\u001b[39mmasked\n\u001b[1;32m   2083\u001b[0m )\n\u001b[0;32m-> 2084\u001b[0m \u001b[43m_gradcheck_real_imag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradcheck_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtupled_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[43m    \u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_grad_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_forward_ad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_forward_ad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_backward_ad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_backward_ad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnondet_tol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnondet_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_undefined_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_undefined_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_batched_forward_grad:\n\u001b[1;32m   2101\u001b[0m     _test_batched_grad_forward_ad(func, tupled_inputs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/gradcheck.py:1494\u001b[0m, in \u001b[0;36m_gradcheck_real_imag\u001b[0;34m(gradcheck_fn, func, func_out, tupled_inputs, outputs, eps, rtol, atol, check_grad_dtypes, check_forward_ad, check_backward_ad, nondet_tol, check_undefined_grad)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         gradcheck_fn(\n\u001b[1;32m   1482\u001b[0m             real_fn,\n\u001b[1;32m   1483\u001b[0m             real_func_out,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1491\u001b[0m             complex_indices\u001b[38;5;241m=\u001b[39mcomplex_out_indices,\n\u001b[1;32m   1492\u001b[0m         )\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1494\u001b[0m         \u001b[43mgradcheck_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtupled_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m            \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m            \u001b[49m\u001b[43matol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcheck_grad_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnondet_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_forward_ad:\n\u001b[1;32m   1507\u001b[0m     complex_inp_indices \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1508\u001b[0m         i\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, inp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tupled_inputs)\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_tensor_like(inp) \u001b[38;5;129;01mand\u001b[39;00m inp\u001b[38;5;241m.\u001b[39mis_complex()\n\u001b[1;32m   1511\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/gradcheck.py:1601\u001b[0m, in \u001b[0;36m_slow_gradcheck\u001b[0;34m(func, func_out, tupled_inputs, outputs, eps, rtol, atol, check_grad_dtypes, nondet_tol, use_forward_ad, complex_indices, test_imag, masked)\u001b[0m\n\u001b[1;32m   1595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _check_no_differentiable_outputs(\n\u001b[1;32m   1596\u001b[0m         func, tupled_inputs, func_out, eps\u001b[38;5;241m=\u001b[39meps, is_forward_ad\u001b[38;5;241m=\u001b[39muse_forward_ad\n\u001b[1;32m   1597\u001b[0m     )\n\u001b[1;32m   1598\u001b[0m tupled_inputs_numerical \u001b[38;5;241m=\u001b[39m tupled_inputs \u001b[38;5;28;01mif\u001b[39;00m masked \u001b[38;5;28;01melse\u001b[39;00m _densify(tupled_inputs)\n\u001b[1;32m   1600\u001b[0m numerical \u001b[38;5;241m=\u001b[39m _transpose(\n\u001b[0;32m-> 1601\u001b[0m     \u001b[43m_get_numerical_jacobian\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtupled_inputs_numerical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_forward_ad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_forward_ad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1608\u001b[0m )\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;66;03m# Note: [numerical vs analytical output length]\u001b[39;00m\n\u001b[1;32m   1610\u001b[0m \u001b[38;5;66;03m# The numerical path returns jacobian quantity for all outputs, even if requires_grad of that\u001b[39;00m\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;66;03m# output is False. This behavior is necessary for _check_no_differentiable_outputs to work.\u001b[39;00m\n\u001b[1;32m   1612\u001b[0m numerical \u001b[38;5;241m=\u001b[39m [nj \u001b[38;5;28;01mfor\u001b[39;00m o, nj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(func_out, numerical) \u001b[38;5;28;01mif\u001b[39;00m o\u001b[38;5;241m.\u001b[39mrequires_grad]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/gradcheck.py:299\u001b[0m, in \u001b[0;36m_get_numerical_jacobian\u001b[0;34m(fn, inputs, outputs, target, eps, is_forward_ad)\u001b[0m\n\u001b[1;32m    294\u001b[0m inp_indices \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    295\u001b[0m     i \u001b[38;5;28;01mfor\u001b[39;00m i, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(target) \u001b[38;5;28;01mif\u001b[39;00m is_tensor_like(a) \u001b[38;5;129;01mand\u001b[39;00m a\u001b[38;5;241m.\u001b[39mrequires_grad\n\u001b[1;32m    296\u001b[0m ]\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inp, inp_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(_iter_tensors(target, \u001b[38;5;28;01mTrue\u001b[39;00m), inp_indices)):\n\u001b[1;32m    298\u001b[0m     jacobians \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 299\u001b[0m         \u001b[43mget_numerical_jacobian_wrt_specific_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[43minp_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m            \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_forward_ad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_forward_ad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m     ]\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jacobians\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/gradcheck.py:491\u001b[0m, in \u001b[0;36mget_numerical_jacobian_wrt_specific_input\u001b[0;34m(fn, input_idx, inputs, outputs, eps, input, is_forward_ad)\u001b[0m\n\u001b[1;32m    485\u001b[0m     jvp_fn \u001b[38;5;241m=\u001b[39m _get_numerical_jvp_fn(\n\u001b[1;32m    486\u001b[0m         wrapped_fn, input_to_perturb, eps, nbhd_checks_fn\n\u001b[1;32m    487\u001b[0m     )\n\u001b[1;32m    488\u001b[0m     jacobian_cols[d_idx] \u001b[38;5;241m=\u001b[39m _compute_numerical_jvps_wrt_specific_input(\n\u001b[1;32m    489\u001b[0m         jvp_fn, eps, x\u001b[38;5;241m.\u001b[39mis_complex(), is_forward_ad\n\u001b[1;32m    490\u001b[0m     )\n\u001b[0;32m--> 491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_combine_jacobian_cols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjacobian_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/gradcheck.py:419\u001b[0m, in \u001b[0;36m_combine_jacobian_cols\u001b[0;34m(jacobians_cols, outputs, input, numel)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_combine_jacobian_cols\u001b[39m(\n\u001b[1;32m    415\u001b[0m     jacobians_cols: Dict[\u001b[38;5;28mint\u001b[39m, List[torch\u001b[38;5;241m.\u001b[39mTensor]], outputs, \u001b[38;5;28minput\u001b[39m, numel\n\u001b[1;32m    416\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;66;03m# jacobian_cols maps column_idx -> output_idx -> single column of jacobian Tensor\u001b[39;00m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;66;03m# we return a list that maps output_idx -> full jacobian Tensor\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     jacobians \u001b[38;5;241m=\u001b[39m \u001b[43m_allocate_jacobians_with_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, jacobian \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(jacobians):\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m jacobians_cols\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/gradcheck.py:74\u001b[0m, in \u001b[0;36m_allocate_jacobians_with_outputs\u001b[0;34m(output_tensors, numel_input, dtype, device)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_allocate_jacobians_with_outputs\u001b[39m(\n\u001b[1;32m     67\u001b[0m     output_tensors: Tuple, numel_input, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# width of `t.numel`. Otherwise, for each tensor, returns a 1-d tensor with size\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# (t.numel,).\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: device, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mstrided}\n\u001b[0;32m---> 74\u001b[0m     out: List[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     75\u001b[0m         t\u001b[38;5;241m.\u001b[39mnew_zeros((numel_input, t\u001b[38;5;241m.\u001b[39mnumel()), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m output_tensors\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_float_or_complex_tensor(t)\n\u001b[1;32m     78\u001b[0m     ]\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/gradcheck.py:75\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_allocate_jacobians_with_outputs\u001b[39m(\n\u001b[1;32m     67\u001b[0m     output_tensors: Tuple, numel_input, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# width of `t.numel`. Otherwise, for each tensor, returns a 1-d tensor with size\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# (t.numel,).\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: device, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mstrided}\n\u001b[1;32m     74\u001b[0m     out: List[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 75\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m output_tensors\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_float_or_complex_tensor(t)\n\u001b[1;32m     78\u001b[0m     ]\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 GiB. GPU 0 has a total capacity of 79.25 GiB of which 13.55 GiB is free. Process 1737049 has 988.00 MiB memory in use. Including non-PyTorch memory, this process has 64.72 GiB memory in use. Of the allocated memory 64.16 GiB is allocated by PyTorch, and 66.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "torch.autograd.gradcheck(Function.apply, (X, A, V, mask), eps=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9103960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
